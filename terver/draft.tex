\section{Случайный вектор}

\opred
Пусть случайный опыт \GOFP.
Случайным вектором $\xI$ размерности $n$, наблюдаемым в опыте $G$, называется упорядоченный набор случайных величин, наблюдаемых в данном опыте.

Можно доказать эквивалентность следующего определения:

\opred
Пусть случайный опыт \GOFP.
Случайным вектором $\xI$ размерности $n$, наблюдаемым в опыте $G$, называется функция $\xI:\Omega\to\R^n$, такая,
что $\xI$ $(\calF,\calB_{\R^n})$-измерима, т. е. $\forall(B\in\calB_{\R^n})[\xI^{-1}(B)\in\calF]$.

\opred
Пусть случайный вектор $\xI$ наблюдается в случайном опыте \GOFP.
Распределением случайного вектора $\xI$ называется функция $P_\xI : \calB_{\R^n} \to [0;1]$, определяемая равенством
$$
P_\xI(B)=P(\xI^{-1}(B))
$$

Можно доказать, что $P_\xI$ --- вероятностная мера на $(\R^n,\calB_{\R^n})$.
Этот факт даёт возможность перейти к выборочному вероятностному пространству (аналогично тому, как это было сделано для случайной величины):
$$
\left<\Omega,\calF,P\right> \stackrel{\xI}{\to} \left<\R^n, \calB_{\R^n}, P_\xI\right>
$$
и рассматривать в нём непосредственно заданный случайный вектор $\vec{\eta}(\vec{x})=\vec{x}$.
Легко видеть, что в таком случае $\forall(B\in\calB_{\R^n})\left[P_{\vec{\eta}}(B)=P_\xI(B)\right]$.
%TODO: примеры







\section{Неравенство Маркова}
Пусть $\xi \in L_1(\Omega,\calF,P)$ и $P\{\xi \geq 0\}=1$, $T>0$.
Тогда 
\begin{equation}\label{neravenstvo Markova}
	P\{\xi \geq T\} \leq \frac{M\xi}{T}
\end{equation}

\dokvo
\begin{multline*}
M\xi = \int_{-\infty}^{\infty} x dF_\xi(x) =
\\
\mbox{(т. к. $\xi$ неотрицательна почти наверное)}
\\
=\int_{\{x \geq T\}} x dF_\xi(x) + \int_{\{0 \leq x < T\}} x dF_\xi(x) \geq 
\\
\mbox{(т. к. $F_\xi$ - неубывающая)} \\
\geq \int_{\{x \geq T\}} x dF_\xi(x) \geq \int_{\{x \geq T\}} T dF_\xi(x)
=T\int_{\{x \geq T\}} dF_\xi(x) = \\ =
T \left( \lim_{x\to+\infty} F_\xi(x) - F_\xi(T-)\right) = T P\{\xi\geq T\}
\end{multline*}

\dokno

\section{Неравенство Чебышева}
Пусть $\xi\in l_2(\Omega,\calF,P)$, $\varepsilon>0$.
Тогда
\begin{equation}\label{neravenstvo_Chebysheva}
P\{|\xi-M\xi|\geq\varepsilon\}\leq\frac{D\xi}{\varepsilon^2}
\end{equation}

\dokvo
\begin{multline*}
	P\{|\xi-M\xi|\geq\varepsilon\}=
	P\{|\xi-M\xi|^2\geq\varepsilon^2\}=
	\\\mbox{(положив в неравенстве Маркова (\ref{neravenstvo Markova}) $T=\varepsilon^2$)}\\
	=\frac{M\left((\xi-M\xi)^2\right)}{\varepsilon^2}
	=\frac{D\xi}{\varepsilon^2}
\end{multline*}


\dokno

\section{Закон больших чисел}

\paragraph{Идеология.}
Обычно случайная величина <<размазана>> по числовой оси.
Если случайные величины складывать, то <<размазанность>> будет <<расползаться>>.
Но оказывается, что при определённых условиях среднее арифметическое величин <<расползаться>> не будет.

\begin{teorema}
Пусть $\left\{\xi_k\right\}_{k=1}^\infty$ --- последовательность стохастически независимых интегрируемых с квадратом случайных величин, дисперсия которых ограничена в совокупности, т. е.
$$
\forall(k\in\N)\left[\xi_k\in L_2(\Omega,\calF,P)\right]
$$
$$
\exists(C>0)\forall(k\in\N)\left[ D\xi_k \leq C \right]
$$

Обозначим $\bar{\xi}_n := \frac{1}{n}\suml_{k=1}^n \xi_k$,
$\bar{\mu}_n := \frac{1}{n}\suml_{k=1}^n M\xi_k$


Тогда 
\begin{equation*}
	\forall(\varepsilon>0)\left[P(|\bar{\xi}_n - \bar{\mu}_n| \geq \varepsilon) \xrightarrow[n\to\infty]{} 0\right]
\end{equation*}
\end{teorema}

\dokvo
\begin{multline*}
	P\{|\bar{\xi}_n-\bar{\mu}_n |\geq \varepsilon\} =
	\\ \mbox{(т. к. $\bar{\mu}_n=M\bar{\xi}_n$)} \\
	=P\{|\bar\xi_n - M\bar\xi_n|\geq \varepsilon\}\leq
	\\\mbox{(применяем неравенство Чебышева (\ref{neravenstvo_Chebysheva}))}\\
	\leq \frac{D\bar\xi_n}{\varepsilon^2}
	=\frac{D\left(\frac{1}{n}\suml_{k=1}^n \xi_k\right)}{\varepsilon^2}
	=\frac{\frac{1}{n^2}D\left(\suml_{k=1}^n \xi_k\right)}{\varepsilon^2}
	=\\\mbox{(в силу стохастической независимости дисперсия аддитивна)}\\
	=\frac{\frac{1}{n^2}\left(\suml_{k=1}^n D\xi_k\right)}{\varepsilon^2}
	\leq \frac{\frac{1}{n^2}\left(\suml_{k=1}^n C\right)}{\varepsilon^2}
	= \frac{nC}{n^2 \varepsilon^2}
	= \frac{C}{n \varepsilon^2}
	\xrightarrow[n\to\infty]{} 0
\end{multline*}
\dokno

\opred
Пусть $\{\xi_k\}_{k=1}^\infty$ --- последовательность случайных величин, наблюдаемых в опыте \GOFP,
$\xi$ --- также случайная величина, наблюдаемая в этом опыте.
Говорят, что $\xi_k$ сходится по вероятности к $\xi$ и пишут:
$$
\xi_k \xrightarrow[n\to\infty]{P} \xi
$$
если
$$
\forall(\varepsilon>0)\left[P\{|\xi_n-\xi|\geq \varepsilon\}\xrightarrow[n\to\infty]{}0\right]
$$

Сформулируем теперь следствие из закона больших чисел --- в случае, когда мы имеем дело с последовательностью одинаково распределённых случайных величин.

\begin{sledstvie}
Рассмотрим последовательность одинаково распределённых интегрируемых с квадратом случайных величин $\{\xi_k\}_{k=1}^\infty$, наблюдаемых в случайном опыте \GOFP.
Обозначим $M\xi_k=\mu$, $D\xi_k=\sigma^2$.
Тогда $\bar\xi_n \xrightarrow[n\to\infty]{P}\mu$.
\end{sledstvie}

Рассмотрим теперь схему Бернулли.

\begin{sledstvie} (теорема Бернулли)
Частота появления события при неограниченном увеличении количества независимых повторений одного и того же опыта по вероятности сходится к вероятности данного события.
Переформулируем строго.

Пусть к \GOFP применена схема Бернулли с вероятностью успеха $p$ и количеством повторений $n$.
Обозначим через $\nu_n$ количество успехов в $n$ опытах.
Тогда $\frac{\nu_n}{n}\xrightarrow[n\to\infty]{P}p$.
\end{sledstvie}

\dokvo
Пусть случайная величина $\xi_k$ равна 1, если в $k$-м опыте произошёл успех, и 0 в противном случае.
Очевидно, что $\xi_k$ стохастически независимы и распределены одинаково.
Заметим, что $\nu_n=\suml_{k=1}^n \xi_k$.
Более того, $\bar\xi_n = \frac{\nu_n}{n}$, $M\xi_k=p$.
Применив следствие 1 из закона больших чисел, получим требуемое.
\dokno

\section{Центральная предельная теорема}

Закон больших чисел и следствия из него позволяют судить о поведении среднего арифметического последовательности одинаково распределённых случайных величин, т. е. сумма величин (обратите внимание, <<сдвинутых>> на матожидание) делится на $n$, благодаря чему и стабилизируется.
Возникает закономерный вопрос: а что будет, если делить не на первую степень $n$, а на небольшую положительную?
Ответ для случая степени, равной $\frac{1}{2}$, и даёт центральная предельная теорема.

\begin{teorema}
Пусть $\{\xi_k\}_{k=1}^\infty$ --- последовательность одинаково распределённых интегрируемых с квадратом случайных величин, наблюдаемых в опыте \GOFP.
Пусть $\xi \sim N(0,1)$.
Обозначим $\mu=M\xi_k$, $\sigma^2=D\xi_k$ ($\sigma>0$).
Проведём теперь над каждой $\xi_k$ манипуляцию, состоящую из уже знакомого нам сдвига на матожидание и новой операции - <<нормирования>> дисперсией:
$$
\xi^0_k=\frac{\xi_k-\mu}{\sigma}
$$
(Рекомендуем, кстати, читателю убедиться, что $\|\xi^0_k\|_{L_2(\Omega,\calF,P)} =1$.)
Тогда $
\bar\xi_n= \frac{ \suml_{k=1}^{n}\xi^0_k }{ \sqrt{n} } 
\xrightarrow [n\to\infty] {\mbox{слабо}} \xi
$, т.е. 
$$
P\{\bar\xi_n<x\}\xrightarrow[n\to\infty]{} \Phi(x)=\frac{1}{\sqrt{2\pi}}\intl_{-\infty}^x e^{-\frac{t^2}{2} }dx
$$
\end{teorema}
\dokvo
В доказательстве будем использовать переход к характеристическим функциям и тот факт, что характеристическая функция суммы равна произведению характеристических функций.

Сначала заметим, что $\dot\varphi_{\xi^0_k} = iM\xi^0_k = 0$, $\ddot\varphi_{\xi^0_k} = i^2 D\xi^0_k = -1$.


\begin{multline*}
\varphi_{\bar\xi_n}(t) = 
\varphi_{\frac{1}{\sqrt{n}}\sum_{k=1}^n \xi^0_k}(t) = 
\varphi_{\sum_{k=1}^n \xi^0_k} \left( \frac{t} {\sqrt{n}} \right) = 
\\ =
\prod_{k=1}^n \varphi_{\xi^0_k}\left( \frac{t} {\sqrt{n}} \right) =
\left(\varphi_{\xi^0_k}\left( \frac{t} {\sqrt{n}}\right) \right)^n =
\\ \mbox{(применяем формулу Тейлора c остаточным членом  в форме Пеано)} \\ =
\left(\varphi_{\xi^0_k}(0) + \dot\varphi_{\xi^0_k}(0) \frac{t}{\sqrt{n}} + \ddot\varphi_{\xi^0_k}(0) \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n =
\\=
\left(1 + 0 \cdot \frac{t}{\sqrt{n}} - 1 \cdot \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n =
\\=
\left(1 - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n
\xrightarrow[n\to\infty]{\mbox{второй замечательный предел}}
e^{-\frac{t^2}{2}} = \varphi_\xi (t)
\end{multline*}

Итак, $\varphi_{\bar\xi_n}(t) \xrightarrow[n\to\infty]{} \varphi_\xi (t)$, следовательно, 
$$
\bar\xi_n = \suml_{k=1}{n}\frac{\xi_k - \mu}{\sigma\sqrt{n}} \xrightarrow[n\to\infty]{} \xi
$$

\dokno

\section{Теорема Муавра-Лапласа}
Особо рассмотрим частный случай центральной предельной теоремы для биномиального распределения.

\begin{teorema}
Пусть $\{\xi_k\}_{k=1}^\infty$ --- последовательность одинаково биномиально с параметрами $(1,p)$ распределённых случайных величин, наблюдаемых в опыте \GOFP.
Тогда
$$
\frac{\suml_{k=1}^n \xi_k - np}{\sqrt{np(1-p)}} \xrightarrow[n\to\infty]{\mbox{слабо}}\xi \sim N(0,1)
$$
\end{teorema}
Для доказательства этой теоремы достаточно вспомнить числовые характеристики биномиального распределения.

\chapter{Типовые распределения}
\section{Распределение Парето}
\subsection{Определение}
\opred
Говорят, что случайная величина $\xi$ имеет распределение Парето с параметрами $x_0>0$ и $\alpha>0$ и пишут $\xi \sim Par(x_0,\alpha)$, если
$$
F_\xi (x) = P\{\xi < x\} = \left(1 - \left(\frac{x_0}{x}\right)^\alpha\right) \cdot \one{[x_0; +\infty)}(x)
$$

Легко видеть, что функция распределения непрерывна.
\subsection{Плотность}
$$
f_\xi (x) = \alpha\frac{x_0^\alpha}{x^{\alpha+1}} \cdot \one{[x_0; +\infty)}(x)
$$

\subsection{Математическое ожидание}
Математическое ожидание, а, следовательно, и другие моменты, могут существовать или не существовать в зависимости от значения $\alpha$.
Попробуем найти матожидание:
\begin{multline}\label{matozhidanie_Pareto}
M\xi=
\intl_{-\infty}^{+\infty}x f_\xi(x) dx =
\intl_{-\infty}^{+\infty}x \alpha \frac{x_0^\alpha}{x^{\alpha+1}} \cdot \one{[x_0; +\infty)}(x) dx =
\intl_{x_0}^{+\infty}x \alpha\frac{x_0^\alpha}{x^{\alpha+1}} dx =
\\=
\intl_{x_0}^{+\infty} \alpha\frac{x_0^\alpha}{x^\alpha} dx = 
\alpha x_0^\alpha \intl_{x_0}^{+\infty} \frac{1}{x^\alpha} dx
\end{multline}
Последний интеграл, как мы знаем из курса математического анализа, сходится при $\alpha>1$.
Следовательно, при $\alpha>1$ из формулы (\ref{matozhidanie_Pareto}) имеем
\begin{multline}\label{matozhidanie_Pareto_itog}
M\xi = 
\alpha x_0^\alpha \intl_{x_0}^{+\infty} \frac{1}{x^\alpha} dx =
\alpha x_0^\alpha \left.\left(\frac{1}{1-\alpha}\frac{1}{x^{\alpha-1}}\right)\right|_{x=x_0}^{x=+\infty} dx =
\\=
\alpha x_0^\alpha\left(0-\frac{1}{1-\alpha}\cdot\frac{1}{x_0^{\alpha-1}}\right)=
\alpha x_0^\alpha\frac{1}{\alpha-1}\cdot\frac{1}{x_0^{\alpha-1}}=
\\=
\frac{\alpha x_0^\alpha}{(\alpha-1)x_0^{\alpha-1}}=
\frac{\alpha x_0}{\alpha-1}
\end{multline}

\subsection{Прочие моменты}
Как известно, начальный момент существует или не существует одновременно с центральным.
Для начального момента порядка $k$ рассуждениями, аналогичными (\ref{matozhidanie_Pareto}), имеем
\begin{equation}\label{momenty_Pareto}
M(\xi^k)= \alpha x_0^\alpha \intl_{x_0}^{+\infty} \frac{1}{x^{\alpha-k+1}} dx
\end{equation}

А такой интеграл сходится при $\alpha-k+1 > 1$, т.е. при $\alpha > k$.
Следовательно, у распределения Парето с параметрами $x_0$ и $\alpha$ существуют $k$-ые центральный и начальный моменты тогда и только тогда, когда $\alpha > k$.

\subsection{Дисперсия}
Вооружившись формулами (\ref{matozhidanie_Pareto_itog}) и (\ref{momenty_Pareto}), посчитаем дисперсию этого распределения при $\alpha>2$:
\begin{multline}
D\xi=
M(\xi^2)-(M\xi)^2 =
\alpha x_0^\alpha \intl_{x_0}^{+\infty} \frac{1}{x^{\alpha-1}} dx - \left(\frac{\alpha x_0}{\alpha-1}\right)^2 =
\\=
\alpha x_0^\alpha \frac{1}{\alpha-2}\cdot\frac{1}{x_0^{\alpha-2}} - \frac{\alpha^2 x_0^2}{(\alpha-1)^2} = 
\frac{\alpha x_0^2}{\alpha-2} - \frac{\alpha^2 x_0^2}{(\alpha-1)^2} = 
\\=
\alpha x_0^2\left(\frac{1}{\alpha-2} - \frac{\alpha}{(\alpha-1)^2}\right) = 
\\=
\alpha x_0^2\left(\frac{(\alpha-1)^2}{(\alpha-2)(\alpha-1)^2} - \frac{\alpha^2-2\alpha}{(\alpha-2)(\alpha-1)^2}\right) =
\\= 
\alpha x_0^2\left(\frac{\alpha^2-2\alpha + 1}{(\alpha-2)(\alpha-1)^2} - \frac{\alpha^2-2\alpha}{(\alpha-2)(\alpha-1)^2}\right) = 
\frac{\alpha x_0^2}{(\alpha-2)(\alpha-1)^2}
\end{multline}

\section{Геометрическое распределение}
\subsection{Определение}
$P\{\xi=k\} = (1-p)^{k-1} p$, $k\in\N$.
\subsection{Математическое ожидание}
\begin{multline}\label{matozhidanie_geom}
M\xi =
\suml_{k=1}^\infty k (1-p)^{k-1} p =
p\suml_{k=1}^\infty k (1-p)^{k-1} =
p\suml_{k=1}^\infty \frac{d}{dp}(-(1-p)^{k}) =
\\=
p\frac{d}{dp} \suml_{k=1}^\infty (-(1-p)^{k}) =
-p\frac{d}{dp} \frac{1-p}{1-(1-p)} = 
-p\frac{d}{dp} \frac{1-p}{p} =
\\=
-p\frac{d}{dp} \left(\frac{1}{p}-1\right) = 
-p\left(-\frac{1}{p^2}\right) =
\frac{1}{p}
\end{multline}

\subsection{Дисперсия}
И снова будем применять почленное дифференцирование рядов.
\begin{multline}
D\xi = 
M(\xi^2) - (M\xi)^2 =
M(\xi^2) - \frac{1}{p^2} =
\suml_{k=1}^\infty k^2 (1-p)^{k-1} p  - \frac{1}{p^2}=
\\=
p\suml_{k=1}^\infty (k^2 + k - k) (1-p)^{k-1}  - \frac{1}{p^2}=
\\=
p\suml_{k=1}^\infty k(k+1)(1-p)^{k-1} - p\suml_{k=1}^\infty k (1-p)^{k-1}  - \frac{1}{p^2}=
\\ \mbox{(значение второй суммы мы уже находили в (\ref{matozhidanie_geom}))} \\=
p\suml_{k=1}^\infty k(k+1)(1-p)^{k-1} - \frac{1}{p}  - \frac{1}{p^2}=
p\suml_{k=1}^\infty \frac{d^2}{dp^2}(1-p)^{k+1} - \frac{1}{p}  - \frac{1}{p^2}=
\\=
p\frac{d^2}{dp^2}\suml_{k=1}^\infty(1-p)^{k+1} - \frac{1}{p}  - \frac{1}{p^2}=
p\frac{d^2}{dp^2}\left((1-p)\suml_{k=1}^\infty(1-p)^{k}\right) - \frac{1}{p}  - \frac{1}{p^2}=
\\=
p\frac{d^2}{dp^2}\left((1-p)\frac{1-p}{p}\right) - \frac{1}{p}  - \frac{1}{p^2}=
p\frac{d^2}{dp^2}\left(\frac{1-2p+p^2}{p}\right) - \frac{1}{p}  - \frac{1}{p^2}=
\\=
p\frac{d^2}{dp^2}\left(\frac{1}{p}-2+p\right) - \frac{1}{p}  - \frac{1}{p^2}=
p\frac{2}{p^3} - \frac{1}{p}  - \frac{1}{p^2}=
\\=
\frac{2}{p^2} - \frac{p}{p^2}  - \frac{1}{p^2}=
\frac{1-p}{p^2}
\end{multline}

\section{Гипергеометрическое распределение}
\subsection{Определение}
$P\{\xi=k\}=\frac{C_M^k C_{N-M}^{n-k}}{C_N^n}$,
где $N$ - общеее количество элементов,
$n$ - выбираемое без возвращения количество элементов,
$k$ - требуемое количество успешных элементов.

\subsection{Математическое ожидание}
Будем, как обычно, полагать, что при $a>b$, или $a<0$, или $b<0$ $C_b^a=0$.
Это позволит нам не следить за пределами суммирования.

Далее, подготовим плацдарм в виде нескольких формул.
Во-первых, сумма вероятностей всех исходов равна 1:
\begin{equation}
\suml_{k=1}^\infty \frac{C_M^k C_{N-M}^{n-k}}{C_N^n} = 1
\end{equation}

Заменив $n$ на $n-1$; $k$ на $k-1$; $M$ на $M-1$; $N$ на $N-1$, имеем:
\begin{equation}\label{summa_gipergeom_1}
\suml_{k=1}^\infty \frac{C_{M-1}^{k-1} C_{N-M}^{n-k}}{C_{N-1}^{n-1}} = 1
\end{equation}

Заметим также, что
\begin{equation}\label{ponizhenie_C}
C_b^a =
\frac{b!}{a!(a-b)!}=
\frac{b(b-1)!}{a(a-1)!((a-1)-(b-1))!}=
\frac{b}{a}C_{b-1}^{a-1}
\end{equation}

Теперь приступаем непосредственно к штурму первого момента.

\begin{multline}
M\xi=
\suml_{k=1}^\infty k\frac{C_M^k C_{N-M}^{n-k}}{C_N^n} \stackrel{(\ref{ponizhenie_C})}{=}
\suml_{k=1}^\infty k\frac{\frac{M}{k} C_{M-1}^{k-1} C_{N-M}^{n-k}}{\frac{N}{n}C_{N-1}^{n-1}}=
\\=
\suml_{k=1}^\infty k\frac{\frac{M}{k} C_{M-1}^{k-1} C_{(N-1)-(M-1)}^{(n-1)-(k-1)}}{\frac{N}{n}C_{N-1}^{n-1}}=
\\ \mbox{(выносим за знак суммы некоторые множители, не зависящие от $k$)} \\=
\frac{Mn}{N}\suml_{k=1}^\infty k\frac{\frac{1}{k} C_{M-1}^{k-1} C_{(N-1)-(M-1)}^{(n-1)-(k-1)}}{C_{N-1}^{n-1}}=
\\=
\frac{Mn}{N}\suml_{k=1}^\infty \frac{ C_{M-1}^{k-1} C_{(N-1)-(M-1)}^{(n-1)-(k-1)}}{C_{N-1}^{n-1}}\stackrel{(\ref{summa_gipergeom_1})}{=}
\frac{Mn}{N}
\end{multline}

